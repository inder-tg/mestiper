# Análisis de forma {#shapeAnalysis}

En esta sección discutiremos algunas ideas alrededor de la interesante tarea
de extraer las principales características de algunas series de tiempo de imágenes
satelitales. Las características que tenemos en consideración van desde los mínimos
y máximos por temporada, hasta los puntos de inflexión en las series anuales. En
el caso de la vegetación, estos puntos de inflexión recientemente han sido considerados
como estimadores de fechas determinantes en el denominado ciclo fenológico.

Para efectos de extraer estas características, visitaremos varios métodos estadísticos
de suavización de curvas y algunos modelos de regresión _no paramétrica_. Consideramos
que no existe una técnica universal para la estimación de las características de cualquier
variable biofísica por lo que el título de este capítulo más que apuntar al uso de
técnicas del denominado _análisis estadístico de forma_, busca catalogar una serie 
de problemas y sus estrategias de solución en un mismo apartado.

Recomendamos asegurarse de incluir las siguientes líneas en el preámbulo de su sesión 
de trabajo en R:

```{r libraries-2}
library(forecast)
library(sta)
```

ya que en algunos de estos paquetes se encuentran las bases de datos que utilizaremos 
en esta sección.

## Smoothing

El paquete ```sta``` contiene el vector ```marismas``` el cual presenta observaciones
del índice de humedad de diferencia normalizada (**NDMI**) tomadas en algún punto del Área
Natural Protegida Marismas durante el periodo 2000-2018, véase la Figura \@ref(fig:marismas).

```{r marismas, echo=FALSE, fig.width=5, fig.height=4, fig.cap = "Serie de tiempo de compuestos a 10 días del NDMI registrada en Marismas Nacionales, México, durante el periodo 2000-2018.", fig.pos="h", out.width='.6\\linewidth'}
marismas_ts <- ts(marismas, start=c(2000,1), end=c(2018,36),
                  frequency=36)

plot(marismas_ts, ylab="NDMI")
```

A largo plazo podemos apreciar la dinámica periódica de este NDMI. Por el momento
concentrémonos en algún año específico, digamos el año 2000.

```{r marismas2000, echo=FALSE}
marismas_ts_2000 <- ts(marismas[1:36], start=c(2000,1), end=c(2000,36), frequency=36)

plot(marismas_ts_2000, ylab="NDMI")
```

Existe un cambio súbito en el valor nominal del NDMI justo después de alcanzar su 
máximo. Además, a partir de esta gráfica -y las observaciones originales- podría
resultar complicado, por ejemplo, establecer el punto en el tiempo en el que el 
valor de NDMI muestra un crecimiento sostenido. Para simplificar las cosas podemos 
optar por aproximar la serie de tiempo original con una versión _más suave_.

La Figura \@ref(fig:marismas-ma) muestra el NDMI del año 2000 y dos suavizadores
por medias móviles. El suavizador por media móvil de orden 2 otorga un aspecto menos
súbito al cambio entre el valor máximo de NDMI y el valor subsecuente. Con el suavizador
por media móvil de orden 6 el cambio mencionado es apenas perceptible, sin embargo,
por definición este suavizador no está definido -no hay valores- para las primeras
y últimas 6 observaciones. 
```{r marismas-ma, echo=FALSE, fig.width=5, fig.height=4, fig.cap = "Compuestos a 10 días del NDMI registrada en Marismas Nacionales en 2000 junto suavizadores de medias móviles.", fig.pos="h", out.width='.6\\linewidth'}
marismas_ts_2000 <- ts(marismas[1:36], start=c(2000,1), end=c(2000,36),
                       frequency=36)

ma_marismas_2000_2 <- ma(marismas_ts_2000, order=2, centre = TRUE)

ma_marismas_2000_4 <- ma(marismas_ts_2000, order=6, centre = TRUE)

yRan <- range(marismas_ts_2000, ma_marismas_2000_4, na.rm = TRUE)

par(mfrow=c(1,2))
par(mar = c(2,2,1,2), adj = 0)
plot(marismas_ts_2000, ylab="NDMI", ylim=yRan, main="MA(2)")
par(new=TRUE)
plot(ma_marismas_2000_2, ylab="NDMI", ylim=yRan, lty=3, col="red")

plot(marismas_ts_2000, ylab="NDMI", ylim=yRan, main="MA(6)")
par(new=TRUE)
plot(ma_marismas_2000_4, ylab="NDMI", ylim=yRan, col="red", lty=3)
```

Numéricamente sería posible determinar mínimo, máximo y hasta puntos de inflexión 
de cualquiera de estos suavizadores. Sin embargo, podemos explotar la estructura
local de las observaciones y aplicar un modelo estadístico a partir del cual realizar
_inferencias_ es una tarea sencilla.

## Regresión armónica

El modelo de regresión armónica es un caso particular del modelo de regresión
lineal por lo que la siguiente discusión sobre la estimación, técnicas de diagnóstico, inferencia y predicción de este modelo están basadas en ideas generales aplicables a cualquier modelo de regresión lineal. 

Típicamente se ha usado el modelo de regresión armónica para extraer las principales características (amplitud, ángulo de fase, parámetros de forma, etc.) de series
de tiempo de NDVI. Por esta razón, para motivar la presentación
de ideas, utilizaremos el modelo de regresión armónica para describir algunas
características de observaciones de NDVI.

### Estimación

Escribiremos $y_t$ para denotar el valor del NDVI al tiempo t. 
Ajustar el modelo de regresión armónica a una serie de $L$ observaciones 
NDVI consiste en estimar los parámetros en la siguiente representación:
\begin{align}
  y_t &= a_0 + 
  \sum_{j=1}^p\,
  \left( a_j \, \cos \left( \frac{2\pi jt}{L} \right) + 
  b_j \, \sin \left( \frac{2\pi jt}{L} \right) \right) +
  \varepsilon_t (\#eq:harmReg-original)\\
  &=
  c_0 +
  \sum_{j=1}^p\,c_j\,\cos\left( \frac{2\pi \, jt}{L} - \varphi_j \right) + \varepsilon_t,
  \quad
  t = 1,\ldots, L. (\#eq:harmReg)
\end{align}
La ecuación \@ref(eq:harmReg) se obtiene haciendo uso de igualdades trigonométricas, un poco de álgebra y definiedo $c_j=\sqrt{ a_j^2 + b_j^2 }$, $\varphi_j=\mbox{arctn}(b_j/a_j)$. Posteriormente discutiremos
las aplicaciones de esta ecuación, en lo que sigue nos 
concentraremos en la Ec. \@ref(eq:harmReg-original). El término $\varepsilon_t$
simboliza el error de medición al tiempo $t$. Este error se supone _aleatorio_ y
en esta sección consideramos que $\varepsilon_t \sim \msc{N}(0,\sigma^2)$.

Los parámetros en \@ref(eq:harmReg-original) son las $a_j$s, $b_j$s y $p$. Si bien existen técnicas de análisis armónico para determinar un valor de $p$, aquí usaremos la evidencia empírica sobre este parámetro y haremos $p=2$. Es decir, la regresión armónica empleada en esta sección utiliza 3 armónicos (el armónico cero dá lugar al parámetro $a_0$). ¿Cómo estimamos las $a_j$s y $b_j$s?

Observa que podemos utilizar algebra matricial para representar las $L$ 
observaciones de NDVI:
\begin{equation}
  \begin{pmatrix}
    y_1 \\ y_2 \\ \vdots \\ y_L
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 
    & 
    \cos \left( \frac{2\pi}{L} \right) & \sin \left( \frac{2\pi}{L} \right) 
    &
    \cos \left( \frac{4\pi}{L} \right) & \sin \left( \frac{4\pi}{L} \right)\\
    1
    &
    \cos \left( \frac{4\pi}{L} \right) & \sin \left( \frac{4\pi}{L} \right) 
    &
    \cos \left( \frac{8\pi}{L} \right) & \sin \left( \frac{8\pi}{L} \right)\\
    \vdots
    & \cdots & \cdots
    & \cdots &\cdots\\
    1
    &
    \cos \left( \frac{2L\pi}{L} \right) & \sin \left( \frac{2L\pi}{L} \right) 
    &
    \cos \left( \frac{4L\pi}{L} \right) & \sin \left( \frac{4L\pi}{L} \right)
  \end{pmatrix}
  \begin{pmatrix}
    a_0 \\ a_1 \\ b_1 \\ a_2 \\ b_2
  \end{pmatrix}
  +
  \begin{pmatrix}
    \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_L
  \end{pmatrix}.
\end{equation}
Más aún, de manera suscinta podemos escribir la Ec. \@ref(eq:harmReg-original) 
de la siguiente forma:
\begin{equation}
  \bs{y}
  =
  \bs{X}\,\bs{\beta} + \bs{\varepsilon},
  (\#eq:harmReg-mat)
\end{equation}
donde $\bs{y}=(y_1,\ldots,y_L)^\top$, $\bs{X}$ se denomina la _matriz de diseño_ 
y en este caso su $k$-ésimo renglón es igual a 
\[
  \bs{X}[k,]
  =
  \begin{pmatrix}
    1 
    & 
    \cos\left( \frac{2\,k\pi}{L} \right) & \sin\left( \frac{2\,k\pi}{L} \right) 
    &
    \cos\left( \frac{4\,k\pi}{L} \right) & \sin\left( \frac{4\,k\pi}{L} \right) 
  \end{pmatrix},
\]
y el vector $\bs{\varepsilon}=(\varepsilon_1,\ldots,\varepsilon_L)^\top$. El
símbolo $\top$ se denomina _transpuesta_.

La presentación de esta notación nos ayudará a simplificar la exposición de 
algunos resultados en las siguientes secciones. Por ahora, respondamos la
pregunta ¿cómo estimamos las $a_j$s y $b_j$s? 

Nota que esta pregunta es equivalente a _estimar_ el vector $\bs{\beta}$ en la
Ec. \@ref(eq:harmReg-mat). Para esto último basta resolver el siguiente problema 
de optimización:
\begin{equation}
  \hat{\bs{\beta}} = \argMin_{\bs{\vartheta}\in \R^5}\,
  \left( \bs{y} - \bs{X}\,\bs{\vartheta} \right)^\top\,
  \left( \bs{y} - \bs{X}\,\bs{\vartheta} \right).
\end{equation}
Esto debe leerse como _$\hat{\bs{\beta}}$ es el vector $\bs{\vartheta}$ 
que minimiza la norma cuadrada de la diferencia entre los vectores $\bs{y}$ y $\bs{X}\bs{\vartheta}$ para todos los posibles valores de $\bs{\vartheta}$ 
como vector de 5 dimensiones._ Pretty cool, ah?

Este problema de optimización tiene el nombre de **mínimos cuadrado ordinarios**
y su solución es ampliamente conocida. En efecto, usando algunas técnicas de 
cálculo diferencial de funciones vectoriales se puede demostrar que
\[
  \hat{\bs{\beta}}
  =
  \left( \bs{X}^\top\,\bs{X} \right)^{-1}\,\bs{X}^\top\,\bs{y}.
\]

Nota que hasta ahora hemos descrito como la Ec. \@ref(eq:harmReg-original) modela 
las $L$ observaciones de NDVI de cualquier año. Podemos denominar a este esquema
como modelación de _mediano plazo_. Bajo este esquema, al usar $p$ armónicos
en \@ref(eq:harmReg-original), el número de parámetros a estimar es $2\times p+1$;
en el ejemplo de arriba, $p=2$ por lo que la longitud del vector $\bs{\beta}$
es 5.

Cuando tenemos valores de NDVI durante $N$ años y por cada año tenemos $L$ 
observaciones de NDVI, tenemos una serie de tiempo de NDVI de tamaño $N \times L$. 
Una manera de modelar las $N \times L$ observaciones de la serie de tiempo, consiste en aplicar \@ref(eq:harmReg-original) a cada uno de los $N$ años bajo consideración.
En este caso hemos de estimar $(2\times p + 1)\times N$ parámetros. Podemos
denominar a este esquema como modelación a _largo plazo_.

Alternativamente, también a largo plazo, podemos modelar las $N\times L$ observaciones en una sola instancia esto debido a la _aparente_
periodicidad de las series de tiempo de NDVI. En este caso, será necesario
que el valor de $p$ sea moderadamente grande; típicamente $p=L$ produce
resultados adecuados.

```{r haRmonics-aux, echo=FALSE}
set.seed(10)
SAMPLE <- sample(1:nrow(mohinora$values), 1)
set.seed(NULL)

y <- mohinora$values[SAMPLE,]
yMat <- vecToMatrix(y,23)
z <- c(t(yMat)) * 1e-4
y <- y * 1e-4
ts_y <- ts(y, start = c(2000, 1), end = c(2009, 23), frequency = 23)

```

Abajo mostramos el ajuste vía regresión armónica de un píxel de NDVI registrado
en la APFF Cerro Mohinora en Chihuahua, México. El periodo estudiado comprende
Enero 1, 2000 a Diciembre 31, 2009. El ajuste fue producido por la función 
```haRmonics``` del paquete ```geoTS```.

::: {.tiny}
```{r haRmonics, echo=c(1:5, 9:26), fig.cap="Ajuste de NDVI (2000-2009) vía regresión armónica con 2 frecuencias.", fig.align='center'}
N <- 10 # Años estudiados
L <- 23 # Total de observaciones por año

fit <- numeric(N * L) 
for(i in 1:N){ # z es un vector con mediciones NDVI
  temp <- haRmonics(y=z[((i-1)*L + 1):(i*L)], method="harmR", 
                    numFreq = 2, delta=0)
  
  fit[((i-1)*L + 1):(i*L)] <- temp$fitted
}

ts_fit <- ts(fit, start = c(2000, 1), end = c(2009,23), frequency = L)

par(mar=c(0,4,0,0))
plot(0,0, type="n", bty="n", xaxt="n", yaxt="n", ylab="")
legend("topleft", legend = c("NDVI", "haRmonics(2)"), 
       lty = c(1,1), col = c("blue", "red"), 
       bty="n", horiz = TRUE, cex=0.75)

par(mar=c(4,4,2,2), new=TRUE)
ts.plot(ts_y, ts_fit, col=c("blue", "red"))
```

:::

### Inferencia estadística

### Bandas de confianza

A continuación, hacemos una presentación breve sobre la construcción de las bandas de 
confianza alrededor de una curva ajustada vía el modelo de regresión armónica.

Al emplear la función ```haRmonics()``` para obtener el ajuste del modelo de regresión armónica, 
en términos abstractos estamos obteniendo un nuevo vector que contiene los valores 
_ajustados_ del modelo; denotemos este vector como $\what{\boldsymbol{y}}$. 

Es conocido que la banda de confianza (al $100(1-\alpha)\%$) alrededor del $k$-ésimo 
valor ajustado, vía un modelo de regresión, tiene la forma
$[\what{\boldsymbol{y}}[k] - \what{\sigma}\,q_k\,z_{1-\alpha/2},
\what{\boldsymbol{y}}[k] + \what{\sigma}\,q_k\,z_{1-\alpha/2}]$,
donde
\[
  \what{\sigma}
  =
  \frac{ \Vert\boldsymbol{y} - \what{ \boldsymbol{y} } \Vert^2 }{L-d}, \quad 
  q_k
  =
  \sqrt{ X[k,] \times \left( X^\top\, X \right)^{-1}\times (X[k,])^\top },
\]
y $z_{1-\alpha/2}$ es el cuantil $1-\frac{\alpha}{2}$ de la distribución normal estándar. 
El parámetro $d$ que aparece en $\what{\sigma}$ es igual al número de parámetros
estimados por el modelo de regresión, en el caso de la regresión armónica, 
Ec. \@ref(eq:harmReg-original), $d=2p+1$. 

Con todo esto, ahora podemos producir bandas de confianza alrededor de los valores ajustados por la regresión armónica:

::: {.tiny}

```{r cbands-harm, echo=c(3:10), fig.cap="Bandas de confianza alrededor de los valores ajustados por la regresión armónica.", fig.align='center'}
fit <- numeric(L*N)

cbands_upper <- numeric(L*N)
cbands_lower <- numeric(L*N)

nFreq <- 2
mat <- getDesignMat(numFreq = nFreq)
w <- getSD(mat)

for(i in 1:10){
  r <- ((i-1)*L + 1):(i*L)
  
  temp <- haRmonics(y=z[r], method="harmR", numFreq=2, delta=0)
  
  fit[r] <- temp$fitted
  
  sigma <- sqrt(sum( ( y[r] - fit[r] )^2 )/( 23 -  (2*nFreq+1) ))
  
  cbands_upper[r] <- fit[r] + sigma * sqrt(w) * qnorm(1-(0.05)/2)
  
  cbands_lower[r] <- fit[r] - sigma * sqrt(w) * qnorm(1-(0.05)/2)
}

ts_fit <- ts(fit, start = c(2000, 1), end = c(2009, 23), frequency = L)
ts_cb_upper <- ts(cbands_upper, start = c(2000, 1), end = c(2009, 23), frequency = L)
ts_cb_lower <- ts(cbands_lower, start = c(2000, 1), end = c(2009, 23), frequency = L)

par(mar=c(0,4,0,0))
plot(0,0, type="n", bty="n", xaxt="n", yaxt="n", ylab="")
legend("topleft", legend = c("haRmonics(2)", "Banda de confianza"), 
       col=c("red", "blue"), lwd=c(1,1), lty=c(1,3),
       bty="n", horiz = TRUE, cex=0.75)

par(mar=c(4,4,2,2), new=TRUE)
ts.plot(ts_fit, ts_cb_lower, ts_cb_upper, col=c("red", "blue", "blue"), lty=c(1,3,3))
```
:::

### Bandas de predicción

Adaptado de @harezlak2018semiparametric. Supongamos que deseamos _predecir_ el
valor de $y_{k_0}$
en una nueva observación temporal para la cual $X[k_0,]$ está disponible. Es decir,
podemos pensar que deseamos conocer el punto $(X[k_0,], y_{k_0})$ cuando
$y_{k_0}=X[k_0,]\boldsymbol{\beta} + \varepsilon$. El predictor es
$\what{y}_{k_0}=X[k_0,]\what{\boldsymbol{\beta}}$ el cual utiliza $X[k_0,]\what{\boldsymbol{\beta}}$ y $0$ para predecir $X[k_0,]\boldsymbol{\beta}$
y $\varepsilon$, respectivamente.^[Notemos que $X[k_0,]\boldsymbol{\beta}$ y $\varepsilon$
son predichas por cantidades que estiman el valor esperado de $X[k_0,]\boldsymbol{\beta}$
y $\varepsilon$: $\ME [X[k_0,]\what{\boldsymbol{\beta}}]=\ME [X[k_0,]\boldsymbol{\beta}]=X[k_0,]\boldsymbol{\beta}$ y $\ME [\varepsilon]=0$.] La incertidumbre en esta predicción proviene de dos fuentes:
$\boldsymbol{\beta}$ será distinto a $\what{\boldsymbol{\beta}}$ y $\varepsilon$ no será
igual a $0$.

Como la diferencia entre los valores esperados de $y_{k_0}$ y $\what{y}_{k_0}$ es igual a 0,
tiene sentido estimar la incertidumbre de la predicción usando $y_{k_0} - \what{y}_{k_0}$.
De este modo, ahora nos interesa conocer la variabilidad de esta cantidad. Para esto,
notamos que $\varepsilon$ es probabilísticamente independiente a $\what{\boldsymbol{\beta}}$
y usamos algunas propiedades conocidas para establecer:
\begin{align*}
  \VAR [ y_{t_0} - \what{y}_{t_0} ]
  &=
  \VAR [ X[k_0,]( \boldsymbol{\beta} - \what{\boldsymbol{\beta}} ) + \varepsilon  ]
  =
  \VAR [ X[k_0,] \what{\boldsymbol{\beta}} ] + \VAR [\varepsilon]\\
  &=
  \sigma^2 \left[ X[k,] \times \left( X^\top\, X \right)^{-1}\times (X[k,])^\top + 1 \right].
\end{align*}

Ahora, estamos en posición de escribir la banda de predicción al $100(1-\alpha)\%$
alrededor de $y_{k_0}$:
\[
  \left[ \what{y}_{k_0} - \what{\sigma} \sqrt{ \VAR [ y_{t_0} - \what{y}_{t_0} ] }\,
  t_{1-\frac{\alpha}{2}, n-d},
  \what{y}_{k_0} + \what{\sigma} \sqrt{ \VAR [ y_{t_0} - \what{y}_{t_0} ] }\,
  t_{1-\frac{\alpha}{2}, n-d}\right],
\]
donde $t_{1-\frac{\alpha}{2}, n-d}$ es el cuantil $1-\frac{\alpha}{2}$ de una
$t$-Student con $n-d$ grados de libertad.

Con todo esto, ahora podemos producir bandas de predicción alrededor de algunos puntos de interés. Por ejemplo, supongamos que no conocemos
el valor de la primera fecha del año 2009 en la serie de tiempo
de NDVI en Cerro Mohinora, con lo escrito arriba ahora podemos
no solo producir un valor predicho sino también una banda de predicción
reflejando la incertidumbre alrededor de este valor estimado.

::: {.small}
```{r predictionBand, echo=c(1,6:8,10:16,23:64), fig.width=8, fig.height=5, fig.cap="Banda de predicción basado en el modelo de regresión armónica (a mediano plazo). Asterisco en rojo marca el valor real del NDVI para la primera fecha de 2010.", fig.align='center'}
fit <- numeric(L)

cbands_upper <- numeric(L)
cbands_lower <- numeric(L)

nFreq <- 2
mat <- getDesignMat(numFreq = nFreq)
w <- getSD(mat)

# ---
i <- 9 # Año a analizar
r <- ((i-1)*L + 1):(i*L) # Observaciones del 9no año
temp <- haRmonics(y=z[r], method="harmR", numFreq=2, delta=0)
fit <- temp$fitted
# ---

sigma <- sqrt(sum( ( y[r] - fit )^2 )/( 23 -  (2*nFreq+1) ))

cbands_upper <- fit + sigma * sqrt(w) * qnorm(1-(0.05)/2)

cbands_lower <- fit - sigma * sqrt(w) * qnorm(1-(0.05)/2)

# ---
FECHA <- 1

# y0 es el predictor de NDVI para la primera fecha
y0 <- sum(temp$amplitude * cos( (2 * pi * c(0:2) * FECHA )/L - temp$phase ))

pb_upper <- y0 + sigma * sqrt(1+w[1]) * qt(1-(0.05)/2, 
                                           df=23-(2*nFreq+1) )
pb_lower <- y0 - sigma * sqrt(1+w[1]) * qt(1-(0.05)/2, 
                                           df=23-(2*nFreq+1) )

LOWER <- c(cbands_lower, pb_lower)
UPPER <- c(cbands_upper, pb_upper)

yRan <- range(UPPER, LOWER, na.rm = TRUE)
yRan[1] <- yRan[1] - 0.05
yRan[2] <- yRan[2] + 0.05

par(mar=c(0,4,0,0))
plot(0,0, type="n", bty="n", xaxt="n", yaxt="n", ylab="")
legend("topleft", legend = c("haRmonics(2)", "Confidence band"),
       lty = c(1,3), col = c("red", "blue"),
       bty="n", horiz = TRUE, cex=0.75)

par(mar=c(4,4,2,2), new=TRUE)
plot(1:24, UPPER, type="l", lty=3, col="blue", ylim=yRan, ylab="",
     xlab="")

points(1:24, c(z[r], y0), pch=20, col="lightgray")
lines(1:24, LOWER, type="l", lty=3, col="blue")
legend("topright", legend = c("Pre. band"), fill = c("#BCD4E6"),
       bty="n")
legend("bottomleft", legend = c("NDVI"), pch=16, col="lightgray",
       cex=0.8, bty="n")

x <- 1:24
polygon( c(x[ x >= 23 ], rev( x[ x >= 23 ] )),
         c(UPPER[ x >= 23 ], rev(LOWER[ x >= 23 ])),
         col = "#BCD4E6", border = NA)

lines(1:24, c(fit,y0), col="red")
points(24, z[208], pch=8, col="red")
```

:::

De acuerdo con la Figura \@ref(fig:predictionBand) la banda de predicción
calculada cubre (marginalmente) el valor real del NDVI de la primera fecha de 2010 (mostrado como un asterisco en rojo).


<!-- ```{r, include=FALSE} -->
<!-- # intersect 2012153_2021121_NDVI_iNDVI.tif y SHP_noQ -->
<!-- manantlan_iNDVI_2012153_2021121_class0 <- stack(tifFILES[1]) -->

<!-- # intersect 2020153_2021091_NDVI_iNDVI.tif y SHP_noQ -->
<!-- manantlan_iNDVI_2020153_2021091_class0 <- stack(tifFILES[2]) -->

<!-- manantlan_iNDVI_2012153_2021121_class0_rTp <- rasterToPoints(manantlan_iNDVI_2012153_2021121_class0) -->

<!-- manantlan_iNDVI_2020153_2021091_class0_rTp <- rasterToPoints(manantlan_iNDVI_2020153_2021091_class0) -->

<!-- yTest <- manantlan_iNDVI_2012153_2021121_class0_rTp[PIXEL,-c(1:2)] -->

<!-- y0_hants_9year <- yTest[97:108] -->

<!-- yTest2 <- manantlan_iNDVI_2020153_2021091_class0_rTp[PIXEL,-c(1:2)] -->

<!-- y0_hants_1year <- yTest2 -->
<!-- ``` -->


<!-- Ahora podemos utilizar la banda de predicción y checar si ésta cubre -->
<!-- los valores de NDVI provistos por otros predictores. Algunos de los predictores considerados a continuación fueron tomados de diferentes archivos otros se obtienen -->
<!-- a partir de los modelos de regresión armónica a mediano y largo plazo discutidos  -->
<!-- en las secciones anteriores. Todos los predictores corresponden al mismo píxel analizado en las secciones anteriores. Los predictores son: -->



<!-- ## Regresión armónica y selección de modelo -->

<!-- En una nota anterior presentamos el modelo de regresión armónica con errores -->
<!-- cuya variabilidad es constante y discutimos criterios empíricos para decidir -->
<!-- el número de frecuencias/armónicos utilizados en el ajuste de este modelo a datos -->
<!-- de NDVI. En esta sección consideramos el mismo modelo de regresión suponiendo que los errores son heteroscedásticos y presentamos el criterio de información de Akaike (AIC en -->
<!-- inglés) el cual nos permite seleccionar el modelo con el mejor balance entre -->
<!-- el número de parámetros y la verosimilitud. -->

<!-- Usemos $y_t$ para denotar el valor del NDVI al tiempo $t$ y consideremos el siguiente modelo -->
<!-- \begin{equation} -->
<!--     y_{t} = f(t) + \varepsilon_t,\qquad t=1,\ldots,L, (\#eq:WLS-HR) -->
<!-- \end{equation} -->
<!-- donde $f$ denota una función que definiremos abajo y suponemos que los errores $\varepsilon_t$ son normales con media cero y varianza $\sigma_t^2$, es decir la varianza toma valores distintos en cada punto del tiempo. -->

<!-- Consideramos que el modelo de regresión armónica con errores heteroscedásticos -->
<!-- intenta describir mejor la dinámica anual del NDVI. Por ejemplo, en el folder -->
<!-- ```/data/phenoParams``` tenemos un dataset con 128 píxeles de NDVI, al tomar  -->
<!-- cualquiera de estos píxeles y graficar su curva de climatología observamos  -->
<!-- características  distintas a lo largo de los 23 días del año (DOY en inglés): -->

<!-- ```{r shape-climatology, fig.width=5, fig.height=4, fig.cap = "Curva de climatología.", fig.pos="h", out.width='.6\\linewidth', echo=FALSE} -->
<!-- path_polygons <- paste0(getwd(), "/data/phenoParams/polygons") -->

<!-- listFiles_polygons <- list.files(path=path_polygons, -->
<!--                                  pattern=".RData", -->
<!--                                  full.names=TRUE) -->

<!-- POLIGONO <- LoadToEnvironment(listFiles_polygons[1])$poly[[1]] * 1e-4 -->

<!-- ROW <- sample(nrow(POLIGONO),1) -->

<!-- pixel_climatology <- climatology(x=POLIGONO[ROW,], lenPeriod=23) -->

<!-- COLORES <- c(brewer.pal(8,"YlGn")[8:5], -->
<!--              brewer.pal(9,"BuGn")[4:1], -->
<!--              brewer.pal(9,"Greens")[7:9], #3 -->
<!--              brewer.pal(9,"YlGn")[9], #1 -->
<!--              brewer.pal(11,"PRGn")[11:9], #3 -->
<!--              brewer.pal(9,"Greens")[7:9], #3 -->
<!--              brewer.pal(9,"BuGn")[9:5]) -->

<!-- par(mar=c(4,3,1,2)) -->
<!-- boxplot(pixel_climatology$matrix, col=COLORES) -->
<!-- ``` -->

<!-- En particular, las varianzas de cada boxplot son considerablemente distintas -la -->
<!-- diferencia entre alguna de ellas alcanza un orden de magnitud- lo cual sustenta -->
<!-- la heteroscedasticidad de los errores en el modelo \@ref(eq:WLS-HR). -->

<!-- ```{r var-climatology, echo¿FALSE} -->
<!-- apply(pixel_climatology$matrix, 2, sd) -->
<!-- ``` -->

<!-- Para la función $f$ consideramos la representación -->
<!-- \[ -->
<!--   f(t)  -->
<!--   =  -->
<!--   \alpha_0  -->
<!--   +  -->
<!--   \sum_{j=1}^p\,\left( \alpha_j\,\cos( \frac{2\pi\,j\,t}{L} )  -->
<!--   +  -->
<!--   \beta_j\,\sin( \frac{2\pi\,j\,t}{L} )  \right) -->
<!-- \] -->
<!-- El valor de $L$ es determinado por el número de observaciones por año, en nuestro -->
<!-- caso $L=23$.  -->


<!-- Definiendo $c_j=\sqrt{\alpha_j^2 + \beta_j^2}$ y  -->
<!-- $\varphi_j=\mbox{arctan}(\beta_j/\alpha_j)$, la representación de $f$ es equivalente -->
<!-- a -->
<!-- \[ -->
<!--   f(t) -->
<!--   = -->
<!--   c_0  -->
<!--   +  -->
<!--   \sum_{j=1}^p\,c_j\,\cos\left( \frac{2\pi\,j\,t}{L} -\varphi_j \right), -->
<!--   \quad -->
<!--   t=1,\ldots,L. -->
<!-- \] -->


<!-- Típicamente, los valores $c_j$ y $\varphi_j$ son llamados _amplitud_ y _ángulo de fase_. -->
<!-- Muchos estudios de NDVI han usado el supuesto $\sigma_t^2=\sigma^2$ -para todo $t$- -->
<!-- para modelar los errores. Además, es común usar $p=3$ or $p=4$, cf. @Eastman.etal.2009,  -->
<!-- para ajustar la función de regresión. -->

<!-- El modelo de regresión armónica con errores heteroscedásticos incorpora información -->
<!-- histórica del NDVI, a través de la estimación de $\sigma_t^2$, de una manera -->
<!-- sencilla la cual contribuye a obtener un mejor ajuste y una selección más transparente -->
<!-- del número de frecuencias/armónicas. -->

### Criterio de información de Akaike.

El modelo \@ref(eq:WLS-HR) puede escribirse en forma matricial
\[
    \bs{Y} = \bs{X}\,\bs{\beta} + \bs{\varepsilon},
\]
donde $\bs{Y}=(y_1,\ldots,y_L)$, $\bs{X}$ es la matriz $(L\times (2p+1))$ de diseño
cuyo $\ell$-ésimo renglón está dado por
\[
  \bs{X}[\ell,]
  =
    \begin{pmatrix}
    1 & \cos( \ell (2\pi)/L ) & \sin( \ell (2\pi)/L ) & \cdots  \cos( \ell (2\pi\, p)/L ) & \sin( \ell (2\pi\, p)/L )
    \end{pmatrix},
\]
y $\bs{\varepsilon}=N(\bs{0},\bs{\Sigma})$, $\bs{\Sigma}=\mbox{diagonal}(\sigma_1^2,\ldots,\sigma_L^2)$-
A partir de esta forma de escribir el modelo, se vuelve muy sencillo presentar
el _criterio de información de Akaike (AIC, en inglés)_ el cual nos permite comparar
entre éste y el AIC del modelo homoscedástico estándar.

Por definición, el AIC de un modelo es
\[
    AIC = 2\,\kappa - 2\,\log\msf{L}(\hat{\vartheta}),
\]
donde $\kappa$ es el número de parámetris del modelo y $\msf{L}$ es la función
de verosimilitud maximizada por el parámetro $\hat{\vartheta}$. Esta cantidad
intenta balancear la cantidad de parámetros en el modelo y la
verosimilitud asociada a los parámetros estimados. Una vez ajustado el modelo,
$\kappa$ es un número fijo (y positivo) y $\log\msf{L}(\hat{\vartheta})$
refleja qué tan plausible es el modelo elegido en función de los parámetros
estimados. Por tanto, mientras más pequeño el AIC -lo cual se debe a que 
la log-verosimilitud es grande- mejor es el modelo seleccionado en el sentido
de balancear número de parámetros vs. plausibilidad.


Denotemos con $\hat{\bs{\gamma}}$ y $\hat{\bs{\beta}}$
los estimadores por mínimos cuadrados ordinarios (OLS) y mínimos cuadrados
ponderados (WLS) de $\bs{\beta}$ en el modelo \@ref(eq:WLS-HR), respectivamente.

En nuestro caso, $\kappa=2\,p+1$ y para el modelo homoscedástico (OLS), 
\[
    L(\hat{\bs{\gamma}}) \propto \sigma^{-L}\, 
    \mbox{exp} \{ -\frac{1}{2\sigma^2}\,\sum_{i=1}^L\,( z_i - [\bs{X}\hat{\bs{\gamma}}][i] )^2 \}.
\]
mientras que para el modelo heteroscedástico (WLS),
\[
    L(\hat{\bs{\beta}}) \propto \prod_{i=1}^L\,\sigma_i^{-1}\, 
    \mbox{exp} \{ -\frac{1}{2}\, \sum_{i=1}^L\, \frac{ (z_i - [\bs{X}\,\hat{\bs{\beta}}][i])^2 }{\sigma_i^2} \}
\]


## Regresión local polinomial

Hemos usado regresión local polinomial como una posibilidad para suavizar datos
de NDVI. En esta sección nos acercamos a este tipo de regresión desde la teoría
general de regresión no paramétrica. Esto nos permitirá dar solución al problema 
de la selección del ancho de banda y el grado del polinomio.

Consideremos el modelo de regresión no paramétrica
\begin{equation}
  y_i 
  =
  f(x_i)
  +
  \sigma(x_i)\,Z,
  \qquad i=1,\ldots,n, (\#eq:NPR)
\end{equation}
donde $f$ tiene derivadas hasta orden $p+1$, $x_i=i/n$, $\sigma(\cdot)$ es una 
función continua en el intervalo $[0,1]$ y $Z \sim N(0,1)$. 

De acuerdo al teorema de Taylor, localmente $f(x)$ puede ser aproximada de la
siguiente forma:
\begin{equation}
  f(x)
  \approx
  f(x_0)
  +
  f^{(1)}(x_0)\,(x-x_0)
  +
  f^{(2)}(x_0)\,(x-x_0)^2/2
  +
  \cdots
  +
  f^{(p)}(x_0)\,(x-x_0)^p/p!, (\#eq:NPR-Taylor)
\end{equation}
donde la distancia entre $x$ y $x_0$ es menor o igual a $h$.^[$f^{(\nu)}(x_0)$ denota
la $\nu$-ésima derivada de $f$ en el punto $x_0$.] 
El valor de $h$ -denominado ancho de banda- define la vecindad, o localidad, 
en donde la aproximación de arriba es válida. 

Escribiendo $\beta_s= f^{(s)}(x_0)/s!$ no es difícil convencerse que 
para estimar $f$ es suficiente resolver un problema de mínimos cuadrados 
_ponderados_. Siendo más precisos, un estimador para $f(x)$ resulta de resolver el siguiente problema de optimización:
\begin{equation}
  \hat{\bs{\beta}}
  =
  \argMin_{ \bs{\beta} }\,
  \sum_{i=1}^n \left( y_i - \sum_{j=0}^p\,\beta_j\, (x_i - x_0)^j \right)^2\,
  K_h( x_i-x_0 ), (\#eq:WLS-NPR)
\end{equation}
donde $K_h(t)=K(t/h)/h$ y $K$ es un _kernel_ satisfaciendo condiciones de 
regularidad estándar. Al igual que hicimos con el modelo de regresión armónica,
ahora usamos notación matricial para representar \@ref(eq:NPR).

Como antes $\bs{Y}=(y_1,\ldots,y_n)$, $\bs{\beta}=(\beta_0,\ldots,\beta_{p})$.
Ahora, la matrix ($n \times (p+1)$) de diseño $\bs{X}$ tiene un $\ell$-ésimo 
renglón igual a
\[
  \bs{X}[\ell,]
  =
  \begin{pmatrix}
    1 & (x_\ell - x_0) & (x_\ell - x_0)^2/2 & \cdots & (x_\ell - x_0)^p/p!\\
  \end{pmatrix}
\]
Como nuevo elemento presentamos la $(n \times n)$ matriz diagonal de pesos
\[
  \bs{W}
  =
  \mbox{diag}( K_h( x_i - x_0 ) ).
\]
 
Es evidente que \@ref(eq:WLS_NPR) es equivalente a encontrar
\[
  \min_{\bs{\beta}} (\bs{Y} - \bs{X}\bs{\beta})^\top \bs{W}\,(\bs{Y} - \bs{X}\bs{\beta})
\]
cuya solución está dada por
\[
  \hat{\bs{\beta}}
  =
  (\bs{X}^\top\,\bs{W}\,\bs{X})^{-1}\,\bs{X}^\top\,\bs{W}\,\bs{Y}.
\]


### Selección de ancho de banda

A partir de la expresión para $\hat{\bs{\beta}}$ la teoría de modelos lineales
nos provee de expresiones para el sesgo y la varianza de $\hat{f}^{(\nu)}(x_0)$ 
-el estimador de la $\nu$-ésima derivada de $f$ en el punto $x_0$. Esas expresiones,
sin embargo, son poco prácticas por lo que algunos grupos de investigadores echaron
mano de papel y lápiz y refinaron esos cálculos. A continuación presentamos
algunos de esos resultados.

Comenzamos definiendo notación. Dado el kernel $K$, tenemos
\[
  \mu_j = \int\,u^j\,K(u)\,du,
\]
objetos que dan lugar a la matriz
\[
  S = ( \mu_{j+\ell} )_{0\leq j,\ell\leq p}.
\]
Consideremos también el vector unitario
$e_{\nu+1}=(0, 0, \ldots, 0, 1, 0, \ldots,0 )^\top$. Estos objetos nos sirven para
definir al denominado _kernel equivalente_
\[
  K_\nu^\ast(t)
  =
  e_{\nu+1}^\top\, S^{-1}\, (1,t,\ldots,t^p)\,K(t).
\]

Ahora podemos presentar el sesgo y varianza asintóticos de $\hat{f}^{(s)}(x_0)$.
A saber, para $s=0,1,\ldots,p$
\[
  \mbox{Sesgo}( \hat{f}^{(s)}(x_0) )
  =
  \left\{ \int\,t^{p+1}\,K_s^\ast(t)\,dt \right\}\,
  \frac{s!}{(p+1)!}\,\hat{f}^{(p+1)}(x_0)\times h^{p+1-s} + o_P( h^{p+1-s} ),
\]
y
\[
  \VAR( \hat{f}^{(s)}(x_0) )
  =
  \int\,K_s^{\ast\,2}(t)\,dt\, \frac{(s!)^2\, \sigma^2(x_0)}{ f(x_0)\, n h^{1+2s} }
  +
  o_P\left( \frac{1}{n h^{1+2s}} \right).
\]

Puede demostrarse que^[Se recomienda ampliamente hacer los cálculos correspondientes.] 
\[
  h_{opt}(x_0)
  =
  C_{s,p}(K)\,
  \left[ \frac{\sigma^2(x_0)}{[ f^{(p+1)}(x_0) ]^2} \right]^{1/(2p+3)}\,
  n^{-1/(2p+3)},
\]
es el valor de $h$ que minimiza el error cuadrático medio (MSE) de 
$\hat{f}^{(s)}(x_0)$.^[Aquí, 
$C_{s,p}(K)=\left[ \frac{((p+1)!)^2\,(2s+1)\,\int K_s^\ast(d)\,dt}{2(p+1-s)[\int\,K_s^\ast(t)\,dt]^2} \right]^{1/(2p+3)}$.]

#### Incremento de variabilidad

El orden de la varianza asintótica dada arriba es $n^{-1}\,h^{-(1+2s)}$.
Esta cantidad no es afectada por el grado del polinomio a ajustar. Sin embargo,
echemos un vistazo a los términos constantes.

Para presentar ideas, concentrémonos en estimar la función de regresión,
es decir $s=0$. En este caso, la varianza asintótica del estimador tiene
la forma
\[
  V_p\,\frac{\sigma^2(x_0)}{f(x_0)\,n\,h}\;\{ 1 + o_P(1) \},
\]
donde $V_p$ es el elemento $(1,1)$ de la matriz $S^{-1}\,S^\ast\,S^{-1}$.
Para $p=0,1,2,3$, los valores de $V_p$ puede obtenerse explícitamente:
\[
  V_0=V_1=\nu_0,
  \qquad 
  V_2=V_3=\frac{ \mu_4^2\,\nu_0 - 2\mu_2\,\mu_4\,\nu_2 + \mu_2^2\,\nu_4 }{(\mu_4 - \mu_2^2)^2}.
\]

Por ejemplo, para el kernel gausiano
\[
  K(z)
  =
  \frac{1}{\sqrt{2\pi}}\mbox{exp}(-z^2/2)
\]
tenemos que
\[
  \mu_{2j}=(2j-1)(2j-3) \cdots 3 \cdot 1\quad
  \mbox{ y }\quad
  \nu_{2j}=2^{-j-1}\mu_{2j}/\sqrt{\pi},
\]
mientras que para el kernel beta simétrico
\[
 K(z)
 =
 \frac{1}{\mbox{Beta}(1/2, \gamma+1)}(1 - t^2)^{\gamma}_{+},\quad \gamma=0,1,\ldots
\]
tenemos que
\[
  \mu_{2j}
  =
  \frac{\mbox{Beta}(j+1/2, \gamma+1)}{\mbox{Beta}(1/2, \gamma+1)},
  \quad
  \mbox{ y }
  \quad
  \nu_{2j}
  =
  \frac{\mbox{Beta}(j+1/2, 2\gamma+1)}{\mbox{Beta}^2(1/2, \gamma+1)}
\]

Seleccionando $\gamma=0,1,2$ y 3 en el kernel beta simétrico arribamos
al kernel uniforme, Epanechnikov, y _biweight_ y _triweight_.

La siguiente figura muestra los valores de $V_p/V_0$ para los kernels
mencionados en el párrafo anterior. Nota que la varianza asintótica aumenta
de un grado impar al consecutivo grado impar. Por ejemplo, en el caso del
kernel Epanechnikov la varianza se incrementa por un factor de 2.0833 cuando
se usa una regresión local cuadrática en lugar de un ajuste linear.

<div class="centered">
  ![Variabilidad de la varianza asintótica del estimador de regresión.](figs/variabilidad_polynomial_order.png){width=25%, height=35%}
</div>

A partir de este análisis parece claro es preferible un ajuste con polinomio
de grado impar. Un ajuste de grado impar $2q+1$ introduce un parámetro extra en 
comparación al ajuste con grado par $2q$, al precio de no incrementar la variabilidad.


### Selección del grado del polinomio

A partir de las expresiones asintóticas de sesgo y varianza de $\hat{f}^{(s)}(x_0)$
presentadas arriba se puede deducir el correspondiente MSE teórico. Algunas
ocasiones conocer el MSE es tarea demandante y no conduce a una solución sencilla
de nuestros problemas de estimación, e.g. determinar de forma sencilla el
ancho de banda. Esto dió origen a otra línea de investigación: estimación de MSE
del estimador de regresión no paramétrica.

En lugar de considerar la expansión de Taylor en \@ref(eq:NPR-Taylor) hasta
orden $p$, extendámosla hasta orden $p+a$ con $a>0$.^[Por practicidad algunos
autores sugieren usar $a=2$.] De modo que podemos calcular el _residual_ -la diferencia- del ajuste original -hasta order p- y el aumentado -hasta orden $p+2$. 
Es decir, $\bs{r}=(r_1,\ldots,r_n)$ donde
\[
  r_i
  =
  \beta_{p+1}\,(x_i-x_0)^{p+1} + \cdots + \beta_{p+a}\,(x_i-x_0)^{p+a},
  \quad
  i=1,\ldots,n.
\]

De acuerdo a lo expuesto en la sección anterior, es fácil ajustar este modelo.
Por completitud, denotamos $\bs{X}^\ast$ y $\bs{W}^\ast$, que similarmente a
$\bs{X}$ y $\bs{W}$, son las matrices de diseño y pesos para el modelo aumentado;
para el modelo aumentado usaremos el ancho de banda $g$.
Esta notación nos permite introducir 
\[
  \hat{\sigma}^{\ast\,2}(x_0)
  =
  \frac{1}{\mbox{tr}(\bs{W}^\ast) - 
  \mbox{tr}( (\bs{X}^{\ast\,\top}\,\bs{W}^\ast\, \bs{X}^\ast)^{-1}\,
  \bs{X}^{\ast\,\top}\,\bs{W}^{\ast 2}\, \bs{X}^\ast
  )}\,
  \sum_{i=1}^{n}\,(y_i - \hat{y}_i)^2\,K\left( \frac{x_i-x_0}{g} \right),
\]
aquí $\hat{y}_i$ es el valor ajustado con el modelo de regresión polinomial
local de grado $p+a$. 

Resulta que al sustituir los estimadores por mínimos cuadrados
de $\beta_{p+1},\ldots,\beta_{p+a}$ en el correspondiente vector de residuales
obtenemos un estimador del sesgo:
\[
  \widehat{\mbox{Sesgo}}_p(x_0)
  =
  (\bs{X}^\top\,\bs{W}\,\bs{X})^{-1}\,
  \bs{X}^\top\,\bs{W}\,\hat{\bs{r}}
  =
  S_n^{-1}\,
  \begin{pmatrix}
  \hat{\beta}_{p+1}^\ast\,s_{n,p+1} + \cdots + \hat{\beta}_{p+a}^\ast\,s_{n,p+a}\\
   \vdots \\
  \hat{\beta}_{p+1}^\ast\,s_{n,2p+1} + \cdots + \hat{\beta}_{p+a}^\ast\,s_{n,2p+a}
  \end{pmatrix},
\]
donde $S_n=\bs{X}^\top\,\bs{W}\,\bs{X}$ es una $(p+1)\times (p+1)$ matriz
cuyo entrada $(i,j)$ es igual a $s_{n,i+j-2}$ con
\[
  s_{n,j}
  =
  \sum_{i=1}^n\,(x_i-x_0)^j\,K( \frac{x_i-x_0}{h}).
\]
Se sugiere hacer $s_{n,p+a+1}=s_{n,p+a+2}=\cdots=s_{n,2a+p}$.

Consideraciones similares arrojan un estimador de la varianza de $\hat{f}^{(s)}(x_0)$.
A saber,
\[
  \widehat{VAR}_p(x_0)
  =
  S_n^{-1}\,( \bs{X}^\top\,\bs{W}^2\,\bs{X} )\,S_n^{-1}\,
  \hat{\sigma}^{\ast 2}(x_0).
\]

Por lo tanto, tenemos una estimador del MSE de $\hat{f}^{(s)}(x_0)$, para
cualquier valor de $s=0,1,\ldots,p$:
\begin{equation}
  \widehat{\mbox{MSE}}_{p,s}(x_0,h)
  =
  (s!)^2\,
  \left( \widehat{\mbox{Sesgo}}^2(x_0) + \widehat{VAR}(x_0) \right). (\#eq:MSE-hat)
\end{equation}

A partir de este estimador del MSE podemos generar estrategias para
determinar el grado óptimo del polinomio local. Una estrategia tal está dada
en el siguiente algoritmo para estimar la $\nu$-ésima derivada de $f$ utilizando
un ancho de banda $h$:

La curva estimada será evaluada en los puntos de la malla 
$\{x_j: j=1,\ldots,n_{malla}\}$. Usaremos $\Delta$ para denotar el ancho de la malla.

  1. Para cada punto $x_j$ de la malla, ajusta un polinomio de grado $p+a$ con
  ancho de banda $g$, obtén los coeficientes 
  $\hat{\beta}^\ast_{\nu+1},\ldots,\hat{\beta}^\ast_{p+a}$ y el estimador
  de la varianza $\hat{\sigma}^{\ast 2}(x_j)$.
  
  2. Para cada grado $\ell$, $\nu+1 \leq \ell\leq p$, utiliza \@ref(eq:MSE-hat) 
  para calcular $\widehat{\mbox{MSE}}_{p,s}(x_0,h)$.
  
  3. Repite 1 y 2 para cada punto de la malla. **Opcionalmente** para cada grado 
  $\ell$ y para cada $x_j$ suaviza el MSE estimado, una posibilidad para suavizar
  es utilizar medias móviles con centro en $x_0$ y ancho de banda $h/\Delta$.
  
  4. Para cada punto $x_j$, elige el grado $\ell_j$ con el menor
  MSE -o su versión suavizada- y utiliza un polinomio de grado $l_j$ para
  estimar $f^{(s)}(x_j)$.

